{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0UntaQv3BtP"
   },
   "source": [
    "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/IntelRealSense/librealsense/master/doc/img/realsense.png\" width=\"70%\" /></p>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The notebook offers a quick hands-on introduction to Intel RealSense Depth-Sensing technology. \n",
    "\n",
    "> **Have a Question?** [Open new issue on our GitHub](https://github.com/IntelRealSense/librealsense/issues/new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZkVvgf-z4D1f"
   },
   "source": [
    "## The Tools\n",
    "We are planning to use the following packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y2jplwqU4OHe",
    "outputId": "7a1685ad-c7ed-4fb9-9d76-eb728609f97f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Ready\n"
     ]
    }
   ],
   "source": [
    "import cv2                                # state of the art computer vision algorithms library\n",
    "import numpy as np                        # fundamental package for scientific computing\n",
    "import matplotlib.pyplot as plt           # 2D plotting library producing publication quality figures\n",
    "import pyrealsense2 as rs                 # Intel RealSense cross-platform open-source API\n",
    "print(\"Environment Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwiY9raM5nCH"
   },
   "source": [
    "## The API\n",
    "Next, we will open depth and RGB streams from pre-recorded file and capture a set of frames:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cV1OkBYn55VH",
    "outputId": "0fbae060-91a8-4f2f-acf8-c20a0cf33dcd"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to resolve request. Request to enable_device_from_file(\"../object_detection.bag\") was invalid, Reason: Failed to create ros reader: Error opening file: ../object_detection.bag",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13836/170397024.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mcfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_device_from_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../object_detection.bag\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Skip 5 first frames to give the Auto-Exposure time to adjust\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to resolve request. Request to enable_device_from_file(\"../object_detection.bag\") was invalid, Reason: Failed to create ros reader: Error opening file: ../object_detection.bag"
     ]
    }
   ],
   "source": [
    "# Setup:\n",
    "pipe = rs.pipeline()\n",
    "cfg = rs.config()\n",
    "cfg.enable_device_from_file(\"../object_detection.bag\")\n",
    "profile = pipe.start(cfg)\n",
    "\n",
    "# Skip 5 first frames to give the Auto-Exposure time to adjust\n",
    "for x in range(5):\n",
    "  pipe.wait_for_frames()\n",
    "  \n",
    "# Store next frameset for later processing:\n",
    "frameset = pipe.wait_for_frames()\n",
    "color_frame = frameset.get_color_frame()\n",
    "depth_frame = frameset.get_depth_frame()\n",
    "\n",
    "# Cleanup:\n",
    "pipe.stop()\n",
    "print(\"Frames Captured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxlrWpLj6mkJ"
   },
   "source": [
    "## RGB Data\n",
    "Let's start with accessing the color componnent of the frameset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "5ajjn7Wq6nEg",
    "outputId": "737674f4-5314-470a-99c0-662f35444e02"
   },
   "outputs": [],
   "source": [
    "color = np.asanyarray(color_frame.get_data())\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.imshow(color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2JsSsF47uZC"
   },
   "source": [
    "## Depth Data\n",
    "Now, we will visualize the depth map captured by the RealSense camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "colab_type": "code",
    "id": "2hF4VSna-6kR",
    "outputId": "d6e9c568-76b9-4b4f-a978-ea91f0c924f2"
   },
   "outputs": [],
   "source": [
    "colorizer = rs.colorizer()\n",
    "colorized_depth = np.asanyarray(colorizer.colorize(depth_frame).get_data())\n",
    "plt.imshow(colorized_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPnn1vivBJgC"
   },
   "source": [
    "## Stream Alignment\n",
    "Upon closer inspection you can notice that the two frames are not captured from the same physical viewport.\n",
    "\n",
    "To combine them into a single RGBD image, let's align depth data to color viewport:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzPfow5oBhyj"
   },
   "outputs": [],
   "source": [
    "# Create alignment primitive with color as its target stream:\n",
    "align = rs.align(rs.stream.color)\n",
    "frameset = align.process(frameset)\n",
    "\n",
    "# Update color and depth frames:\n",
    "aligned_depth_frame = frameset.get_depth_frame()\n",
    "colorized_depth = np.asanyarray(colorizer.colorize(aligned_depth_frame).get_data())\n",
    "\n",
    "# Show the two frames together:\n",
    "images = np.hstack((color, colorized_depth))\n",
    "plt.imshow(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDPAZ2wvCg50"
   },
   "source": [
    "Now the two images are pixel-perfect aligned and you can use depth data just like you would any of the other channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4JM4Fz0vhoj0"
   },
   "source": [
    "## Object Detection\n",
    "\n",
    "Next, we will take advantage of widely popular **MobileNet SSD Model** to recognize and localize objects in the scene and use additional depth data to enrich our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "z4sPyGkWY_co",
    "outputId": "5d6dd74a-4986-4c63-deee-cf1cc95610f8"
   },
   "outputs": [],
   "source": [
    "# Standard OpenCV boilerplate for running the net:\n",
    "height, width = color.shape[:2]\n",
    "expected = 300\n",
    "aspect = width / height\n",
    "resized_image = cv2.resize(color, (round(expected * aspect), expected))\n",
    "crop_start = round(expected * (aspect - 1) / 2)\n",
    "crop_img = resized_image[0:expected, crop_start:crop_start+expected]\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(\"../MobileNetSSD_deploy.prototxt\", \"../MobileNetSSD_deploy.caffemodel\")\n",
    "inScaleFactor = 0.007843\n",
    "meanVal       = 127.53\n",
    "classNames = (\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "              \"bottle\", \"bus\", \"car\", \"cat\", \"chair\",\n",
    "              \"cow\", \"diningtable\", \"dog\", \"horse\",\n",
    "              \"motorbike\", \"person\", \"pottedplant\",\n",
    "              \"sheep\", \"sofa\", \"train\", \"tvmonitor\")\n",
    "\n",
    "blob = cv2.dnn.blobFromImage(crop_img, inScaleFactor, (expected, expected), meanVal, False)\n",
    "net.setInput(blob, \"data\")\n",
    "detections = net.forward(\"detection_out\")\n",
    "\n",
    "label = detections[0,0,0,1]\n",
    "conf  = detections[0,0,0,2]\n",
    "xmin  = detections[0,0,0,3]\n",
    "ymin  = detections[0,0,0,4]\n",
    "xmax  = detections[0,0,0,5]\n",
    "ymax  = detections[0,0,0,6]\n",
    "\n",
    "className = classNames[int(label)]\n",
    "\n",
    "cv2.rectangle(crop_img, (int(xmin * expected), int(ymin * expected)), \n",
    "             (int(xmax * expected), int(ymax * expected)), (255, 255, 255), 2)\n",
    "cv2.putText(crop_img, className, \n",
    "            (int(xmin * expected), int(ymin * expected) - 5),\n",
    "            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255,255,255))\n",
    "\n",
    "plt.imshow(crop_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyECTYucnjB_"
   },
   "source": [
    "By projecting this data into the depth channel, we can now answer additional questions we couldn't approach before.\n",
    "\n",
    "For example, with computer vision only it would be rather hard to make any meaningful predictions about **size and distance**.\n",
    "You could train a model on average dog size per breed, but it would be easily fooled by toys of dogs or dogs of irregular proportions. Instead you can get this information directly when you have depth available!\n",
    "\n",
    "Let's project our detected bounding box on to the depth image, and average the depth data inside it to get a sense of how close is the object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "colab_type": "code",
    "id": "l3fW-u7mlo3V",
    "outputId": "5904a9ae-bed7-4bf8-ce2d-144750851b07"
   },
   "outputs": [],
   "source": [
    "scale = height / expected\n",
    "xmin_depth = int((xmin * expected + crop_start) * scale)\n",
    "ymin_depth = int((ymin * expected) * scale)\n",
    "xmax_depth = int((xmax * expected + crop_start) * scale)\n",
    "ymax_depth = int((ymax * expected) * scale)\n",
    "xmin_depth,ymin_depth,xmax_depth,ymax_depth\n",
    "cv2.rectangle(colorized_depth, (xmin_depth, ymin_depth), \n",
    "             (xmax_depth, ymax_depth), (255, 255, 255), 2)\n",
    "plt.imshow(colorized_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cz-mgKk7l-xm",
    "outputId": "08b3f153-ad66-4d28-c58d-de24064d5f8f"
   },
   "outputs": [],
   "source": [
    "depth = np.asanyarray(aligned_depth_frame.get_data())\n",
    "# Crop depth data:\n",
    "depth = depth[xmin_depth:xmax_depth,ymin_depth:ymax_depth].astype(float)\n",
    "\n",
    "# Get data scale from the device and convert to meters\n",
    "depth_scale = profile.get_device().first_depth_sensor().get_depth_scale()\n",
    "depth = depth * depth_scale\n",
    "dist,_,_,_ = cv2.mean(depth)\n",
    "print(\"Detected a {0} {1:.3} meters away.\".format(className, dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4TPljtWrUlw"
   },
   "source": [
    "This is a rather simple example, but it gives you a taste of what can be accomplished by combining depth with modern computer vision.\n",
    "\n",
    "> **Want to learn more?** Visit [realsense.intel.com](http://realsense.intel.com) and [github.com/IntelRealSense](http://www.github.com/IntelRealSense/librealsense)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Intel RealSense API",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "8ce51fb231350df207c05c2352339f5b129ccc325d9125005499071f447f3ecc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
